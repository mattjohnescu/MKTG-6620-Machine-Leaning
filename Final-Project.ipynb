{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Neural Network \n",
    "\n",
    "- PCA and Clustering\n",
    "\n",
    "- Ensemble Method\n",
    "How are feature correlations different from feature importance? Is there a way to determine feature importance for random forest similar to boosted trees? What do the features mean in the context of your question? Why didn't you tune your models? This is usually a requirement for any boosted or bagging method. RMSE is a much better indicator of goodness of fit than R squared so focus on this. RMSE is MUCH smaller for boosted trees. Why? Is this in the same scale or did the package maybe standardize your data? \n",
    "\n",
    "\n",
    "- Penalized Regression \n",
    "Good dataset for the task and appropriate implementation. What are your coefficients and what do they mean? How are you supposed to know the relationship between your independent and dependent variables if you don't print your summaries? What does an MSE of 1.89 mean? Think deeper about your limitations. More important than the limitations of the model are the limitations of your data. How is the data collected? Which variables are included? What do these tell you about how you can apply your analysis in the real world? And for robustness checks, these aren't robustness checks of the method like CV hyperparameter tuning. These should be acknowledging what decisions you made (for example, how to deal with unknown) and whether your conclusions change based on these decisions.\n",
    "\n",
    "\n",
    "- SVM \n",
    "Why didn't you do cross validation to select your hyperparameters? Gamma is also a hyperparameter for rbf but you didn't even specify that. You could also have specified a metric suitable to an imbalanced dataset like F1. None of that would have taken very long and would have allowed you to create a functional model. As is, just guessing no for every test item would be about twice as accurate as your model. Make sure to address how to ensure your conclusions are robust, not just your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [Important Packages](#importing-packages)\n",
    "- [Loading Data](#loading-data)\n",
    "- [Data Exploration](#data-exploration)\n",
    "- [Cleaning Data](#cleaning-data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add File Path of the CSV file here\n",
    "file_path = \"C:/Users/johne/Downloads/Electronic_sales_Sep2023-Sep2024.csv\"\n",
    "# file_path = \"C:/Users/matt/Downloads/archive (2)/Electronic_sales_Sep2023-Sep2024.csv\"\n",
    "# Reading the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
